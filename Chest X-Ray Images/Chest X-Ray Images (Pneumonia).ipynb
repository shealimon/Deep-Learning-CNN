{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## Data Preparation and Analysis\n\nThe first portion of the code will be dedicated to preparing the data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom keras.preprocessing.image import ImageDataGenerator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first few lines are importing the libraries we will need for preparing and visualizing our data."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path = \"../input/chest_xray/chest_xray\"\ndirs  = os.listdir(path)\nprint(dirs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are setting the path of the chest_xray folder for later use. We are then printing out the directories from within the chest_xray folder. Notice that the folder is split into three subfolders: test, train and val, or validation. Each folder contains chest X-ray images that we will need to use for training and testing."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_folder = path + '/train/'\ntest_folder  = path + '/test/'\nval_folder   = path + '/val/'\n\ntrain_dirs = os.listdir(train_folder)\nprint(train_dirs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will set the paths for each folder. We can use the \"path\" variable we set earlier and concatenate that with each subfolder's name. Then we will want to see what is in the training folder. To view the directories, we will use the `listdir()` function for the training folder, then print the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_normal = train_folder + 'NORMAL/'\ntrain_pneu   = train_folder + 'PNEUMONIA/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can then take our training folder and and set the paths to each class. In this case, we have two classes: The <b>normal</b> images and the <b>pneumonia</b> images. If we want to visualize images that are specifically \"normal\" or \"pneumonia\", then we will create a variable that contains the path to these images for later reference."},{"metadata":{"trusted":true},"cell_type":"code","source":"pneu_images   = glob(train_pneu + \"*.jpeg\")\nnormal_images = glob(train_normal + \"*.jpeg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have split the training folder into \"normal\" and \"pneumonia\", we can pull all of the images out of each class. The images in this dataset are all jpeg images, so for each path we will add `.jpeg` at the end to make sure we are pulling out the images. The reason for this is because we want to get a visual of our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_imgs(num_of_imgs):\n    \n    for img in range(num_of_imgs):\n        pneu_pic   = np.asarray(plt.imread(pneu_images[img]))\n        normal_pic = np.asarray(plt.imread(normal_images[img]))\n\n        fig = plt.figure(figsize= (15,10))\n\n        normal_plot = fig.add_subplot(1,2,1)\n        plt.imshow(normal_pic, cmap='gray')\n        normal_plot.set_title('Normal')\n        plt.axis('off')\n\n        pneu_plot = fig.add_subplot(1, 2, 2)\n        plt.imshow(pneu_pic, cmap='gray')\n        pneu_plot.set_title('Pneumonia')\n        plt.axis('off')\n    \n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will create a function called `show_imgs()` to visualize the chest X-ray images from within our training set. the function will take one argument that specifies how many images to show (`num_of_imgs`). We will then use a for loop with a range of \"num_of_imgs\" to show however many images is specified. \n\nWe will be showing <b>normal</b> images and <b>pneumonia</b> images side by side so we will add two sub plots: one for normal, one for pneumonia. The color map for these images will be 'grays'. If you feel like changing the color map, head over to Matplotlb's [color map reference](https://matplotlib.org/examples/color/colormaps_reference.html) page. For each image shown, we will label it as either \"normal\" or \"pneumonia\" by setting the sub plot's title. "},{"metadata":{"trusted":true},"cell_type":"code","source":"show_imgs(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use our `show_imgs()` function like this. We will call the function and give it one argument: the number of images of both classes we would like to show.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale            = 1/255,\n                                   shear_range        = 0.2,\n                                   zoom_range         = 0.2,\n                                   horizontal_flip    = True,\n                                   rotation_range     = 40,\n                                   width_shift_range  = 0.2,\n                                   height_shift_range = 0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is called <b>image preprocessing</b>, or <b>data augmentation</b>. We will be using the `ImageDataGenerator()` class from Keras for our data augmentation. Data augmentation helps us to expand our training dataset. The more training data the better. With more training data, overfitting becomes less of a problem as our model has to generalize more. \n\n* The first step is to rescale our data. Rescaling images is a common practice because most images have RGB values ranging from 0-255. These values are too high for most models to handle, but by multiplying these values by 1/255, we can condense each RGB value to a value between 0-1. This is much easier for our model to process.\n\n* Next we have `shear_range` which will randomly apply shear mapping, or shear transformations to the data. The value \"0.2\" is the [shear](https://en.wikipedia.org/wiki/Shear_mapping) intensity, or shear angle.\n\n* `zoom_range` is also set to \"0.2\". This is for randomly zooming in on the images.\n\n* `horizontal_flip` is set to \"True\" because we want to randomly flip half of the images in our dataset. \n* `rotation_range` is the value in degrees for which the image may be randomly rotated. \n* `width_shift_range` and `height_shift_range` are ranges for randomly translating images. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_datagen = ImageDataGenerator(rescale = 1/255)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is where we rescale our test set. The test set does not need all of the same transformations applied to the training data. Only the training data can be manipulated to avoid overfitting. The test set must be the original images because we want to accurately predict pneumonia on real, minimally manipulated images."},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = train_datagen.flow_from_directory(train_folder,\n                                   target_size= (64, 64),\n                                   batch_size = 32,\n                                   class_mode = 'binary')\n\nval_set = test_datagen.flow_from_directory(val_folder,\n                                   target_size=(64, 64),\n                                   batch_size = 32,\n                                   class_mode ='binary')\n\ntest_set = test_datagen.flow_from_directory(test_folder,\n                                   target_size= (64, 64),\n                                   batch_size = 32,\n                                   class_mode = 'binary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will take the path of our test, train, and validation folders and generate batches of augmented data using `flow_from_directory()` from Keras. \n* The first argument will be the directory to pull from.\n* The second argument is the target size, or the dimensions of the images after they are resized.\n* The third argument is \"class_mode\", which is set to \"binary\". This will return 1D binary labels. \n\nNow that we are done preparing our data, we can move on to building the model, training it and then testing it and getting our results in the form of accuracy scores."},{"metadata":{},"cell_type":"markdown","source":"## Applying CNNs to Predicting Pneumonia"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3), padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the exciting part. I will break down exactly what is happening in each line/layer below:\n\n* First, we create our model using the \"Sequential\" model from Keras. This model is a linear stack of layers, meaning that we will create our model layer-by-layer. \n\n\n* <b>1st Convolutional Layer</b>: The first convolutional layer is our input layer. The first parameter is the amount of convolutional filters to use in the layer, which is set to \"32\". This is also the number of neurons, or nodes, that will be in this layer.\n\n     The second parameter is the filter's size, or the receptive field. Imagine we are creating a window of the size (3, 3), or a width of three and a height of three, that our convolutional layer is restricted to looking through at any given time.\n     \n     The third parameter we will set is the activation function. Our nonlinear activation function is ReLu, or rectified linear unit. The ReLu function is <i>`f(x) = max(0, x)`</i>. Therefore, all negatives are converted to zeros while all positives remain the same. ReLu is one of the most popular activation functions because it reduces the the vanishing gradient issue and is computationally cheaper to compute. This does not mean that the ReLu function is perfect, but it will get the job done for most applications. \n     \n     The fourth parameter is the input shape. This parameter <b>only needs to be specified in the first convolutional layer</b>. After the first layer, our model can handle the rest. The input shape is simply the shape of the images that will be fed to the CNN. The shape of our input images will be (64, 64, 3) (width, height, depth). \n     \n     The final parameter is the padding, which is set to \"same\". This will pad the input in a way that makes the output have the same length as the initial input.  \n\n\n* <b>1st Max Pooling Layer</b>: The max pooling layers will only have on parameter for this model. The parameter is the pool size, or the factor to downscale the input's spatial dimensions. The pool size will be set to (2, 2), which will downscale by half each time. Refer to the earlier section \"A Brief Introduction to a CNN's Architecture\" for more details about the pooling layer.\n\n\n* <b>2nd Convolutional and Max Pooling Layer</b>: The second convolutional layer and max pooling layer will be the same as the previous layers above. The second convolutional layer will not need the input size to be specified.\n\n\n* <b>3rd Convolutional Layer</b>: In the third convolutional layer, the first parameter will be changed. In the first two convolutional layers, the number of filters, or neurons in the layer, was set to \"32\", but for the thrid layer it will be set to \"64\". Other than this one change, everything else will stay the same.\n\n\n* <b>3rd Max Pooling Layer</b>: The third max pooling layer will be the same as the first two previous pooling layers.\n\n\n* <b>Flatten</b>: Flattening is required to convert multi-dimensional data into usable data for the fully connected layers. In order for the fully connected layers to work, we need to convert the convolutional layer's output to a 1D vector. Our convolutional layers will be using 2D data (images). This will have to be reshaped, or flattened, to one dimension before it is fed into the classifier. \n\n    If we take a look at a portion of the model summary, the output data of the third max pooling layer has a shape of `(None, 6, 6, 64)`. The output shape after flattening is `(None, 2304)`. This is because (6 * 6 * 64) = 2304.\n    ```\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    max_pooling2d_16 (MaxPooling (None, 6, 6, 64)          0         \n    _________________________________________________________________\n    flatten_5 (Flatten)          (None, 2304)              0         \n    _________________________________________________________________\n    ```\n\n* <b>Dense - ReLu</b>: Dense layers are the fully connected layers, meaning that every neuron is connected to all the neurons in previous layers. We will be using 128 nodes. This also means that the fully connected layer with have an output size of 128. For this fully connected layer, the ReLu activation function will be used.\n\n\n* <b>Dropout</b>: Dropout is used to regularize our model and reduce overfitting. Dropout will temporarily \"drop out\" random nodes in the fully connected layers. This dropping out of nodes will result in a thinned neural network that consists of the nodes that were not dropped. Dropout reduces overfitting and helps the model generalize due to the fact that no specific node can be 100% reliable. The \".5\" means that the probability of a certain node being dropped is 50%. To read more about dropout, check out [this paper](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf).\n\n\n* <b>Dense - Sigmoid</b>: Our final fully connected layer will use the <b>sigmoid function</b>. Our problem involves two classes: Pneumonia and normal. This is a binary classification problem where sigmoid can be used to return a probability between 0 and 1. If this were a multi-class classification, the sigmoid activation function would not be the weapon of choice. However, for this simple model, the sigmoid function works just fine. The sigmoid function can be defined as:\n\n<center><img src=\"https://latex.codecogs.com/png.latex?\\dpi{120}&space;f(x)=\\frac{1}{1&space;&plus;&space;e^{-x}}\" title=\"f(x)=\\frac{1}{1 + e^{-x}}\" /></center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now configure the model using the compile method from Keras.\n\n* The first argument is the optimizer which will be set to \"adam\". The adam optimizer is one of the most popular algorithms in deep learning right now due to the results it produces. The authors of [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8) state that Adam combines the advantages of two other popular optimizers: RMSProp and AdaGrad. You can read about the effectiveness of Adam for CNNs in section 6.3 of the Adam paper.\n\n\n* The second argument is the loss function. This model will use the <b>binary cross entropy</b> loss function. Our model will be conducting binary classification, so we can write this loss function as shown below, where \"y\" is either 0 or 1, indicating if the class label is the correct classification and where \"p\" is the model's predicted probability:\n\n<center><img src=\"https://latex.codecogs.com/gif.latex?-(y\\log(p)&space;&plus;&space;(1&space;-&space;y)\\log(1&space;-&space;p))\" title=\"-(y\\log(p) + (1 - y)\\log(1 - p))\" /></center>\n\n\n* The last argument is the metric function that will judge the performance of the model. In this case, we want the accuracy to be returned."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_train = model.fit_generator(training_set,\n                         steps_per_epoch = 200,\n                         epochs = 5,\n                         validation_data = val_set,\n                         validation_steps = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is now time to train the model! This will be done using the `fit_generator()` method from Keras. This will train the model on batches of data that are generated from the training set. \n\n* The first argument is the steps per epoch. This will be set to 200. The steps per epoch will tell the model the total number of batches of samples to produce from the generator before concluding that specific epoch. \n\n\n* The second argument is the number of epochs, or training iterations. The Keras  documentation states that an epoch is defined as an iteration over the entire data provided, as defined by steps_per_epoch. \n\n\n* The third argument is the validation data the model will use. The model will not be trained on the validation data, but this will help measure the loss at the end of every epoch.\n\n\n* The final argument is the validation steps. our validation data is coming from a generator (see above code), so the number of batches of samples to produce from the generator must be set, similar to the steps per epoch. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_accuracy = model.evaluate_generator(test_set,steps=624)\n\nprint('Testing Accuracy: {:.2f}%'.format(test_accuracy[1] * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that the model has been trained, it is time to evaluate the model's accuracy on the test data. This will be done by using the evaluate_generator method from Keras. This evaluation will return the test loss and accuracy results.\n* Just like the fit generator, the first argument for the evaluate generator is the folder from which to pull samples from. Since we are testing our model's accuracy, the test set will be used.\n\n\n* The second argument is the number of batches of samples to pull from the generator before finishing. \n\nWe can then print the accuracy and shorten it to only show two decimal places. The accuracy will be returned as a value between 0-1, so we will multiply it by 100 to recieve the percentage."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}